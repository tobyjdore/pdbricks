import pandas as pd
from databricks import sql
import io


def _format_sql_host(host):
    '''
    Helper function to ensure host is properly formatted,
    as the SQL connection follows a different protocol to the FileStore connection
    '''
    
    return host.replace('https://','').rstrip('/')


def query_sql(host,
              cluster_path,
              token,
              query):
    '''
    Query SQL table and return as a pandas dataframe
    
    Parameters
    ----------
    host : the DataBricks instance you're trying to connect to; [instance].azuredatabricks.net
    cluster_path : the HTTP path to the DataBricks cluster, found in Computer > [instance] Advanced Options > JDBC/ODBC
    token : an access token as a string
    query : SQL query string
    '''
    
    conn = sql.connect(_format_sql_host(host),cluster_path,token)
    
    cursor = conn.cursor()
    
    cursor.execute(query)
    results = cursor.fetchall()
    
    cols = [x[0] for x in cursor.description]
    
    cursor.close()
    conn.close()
    
    df = pd.DataFrame(results,columns=cols)
    
    return df


def create_table_from_file(host,
                           cluster_path,
                           token,
                           file_name,
                           table_name,
                           cols,
                           dtypes,
                           db_name='default'):
    '''
    Create a table from an uploaded file.
    Quicker to do it this way than to try to insert table directly
    Main function in the class will call DBFS functions
    
    Parameters
    ----------
    host : the DataBricks instance you're trying to connect to; [instance].azuredatabricks.net
    cluster_path : the HTTP path to the DataBricks cluster, found in Computer > [instance] Advanced Options > JDBC/ODBC
    token : an access token as a string
    file_name : file to copy from, this will be autogenerated when we upload the df
    table_name : table to write to
    cols : column names for table
    dtypes : datatypes for columns (DataBricks formatted, converted from pandas)
    db_name : name of the database to create the table in (default = 'default')
    '''
    
    col_string = ', '.join(['{} {}'.format(a,b) for a,b in zip(cols,dtypes)])
    
    drop_query = '''
    DROP TABLE IF EXISTS {}.{}
    '''.format(db_name,table_name)

    create_using_query = '''
    CREATE TABLE {}.{} ({}) USING CSV LOCATION '{}'
    '''.format(db_name,table_name,col_string,file_name)
    
    conn = sql.connect(_format_sql_host(host),cluster_path,token)
    
    cursor = conn.cursor()
    
    cursor.execute(drop_query)
    cursor.execute(create_using_query)
    
    cursor.close()
    
    conn.close()